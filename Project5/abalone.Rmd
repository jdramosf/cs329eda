---
title: "Data Analytics Final Project: Abalone"
author: "Ananya Kaushik, Natasha Pirani, Nolan Bentley, and Juan Ramos Fuentes"
resource_files:
- .Renviron
output:
  html_notebook:
    code_folding: hide
    toc: yes
    toc_depth: 4
    toc_float: yes
runtime: shiny
---

```{r setup, include=FALSE}
dwapi::configure(auth_token = Sys.getenv("DW_API"))
library(tidyverse)
library(class)
require(data.world)
require(ggplot2)
require(shiny)
require(dplyr)
require(ISLR)
require(MASS)
require(vcd)
require(plotly)
require(boot)
require(leaps)
require(tree)
require(randomForest)
require(gbm)
require(glmnet)
require(e1071)
packageVersion('plotly')
knitr::opts_chunk$set(echo = TRUE)
options(shiny.sanitize.errors = FALSE)
```

## **Setting Up the Environment**  
### **R Session Info**
```{r, echo=FALSE, message=FALSE, warning=FALSE}
sessionInfo()
```

### **data.world Project Link**
https://data.world/ananya-kaushik/f-17-eda-project-5

### **Connecting to data.world**
```{r, echo=TRUE, message=FALSE, warning=FALSE}
project = "https://data.world/ananya-kaushik/f-17-eda-project-5"
abalone <- data.world::query(
  data.world::qry_sql("select * from abalone_data"),
  dataset = project
)

colnames(abalone) <- c("sex", "length", "diameter", "height", "wholeweight", "shuckedweight", "visceraweight", "shellweight", "rings")
abalone$sex = as.factor(abalone$sex)
attach(abalone)
```

# **Introduction**
This document contains an analysis on abalones. We used different statistical analysis methods to predict the number of rings in an abalone using different physical measures. The number of rings plus 1.5 gives the age of the abalone in years. The age of abalone is usually determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a boring and time-consuming task. We hope that, through our analysis, we can simplify this process.

## **Dataset -  Abalone**
```{r, warning=FALSE}
renderDataTable( options = list(pageLength = 10), {
  abalone
})
```


# **Statistical Analysis - Supervised**
## **[Multi-predictor Linear Regression Using Cross Validation](https://data.world/ananya-kaushik/f-17-eda-project-5/insights/126d3136-f451-49a5-a179-85af71452de7)** 

Let's see if we can identify strong relationships between the number of rings found in an abalone and the rest of features collected on this dataset.
[Pairs Plot](https://data.world/ananya-kaushik/f-17-eda-project-5/insights/1c391d4a-1022-45b0-9abf-962bd4253737)

```{r, warning=FALSE}
renderPlot({
  pairs(abalone[2:9])
})
```

We exclude sex because is a qualitative variable.
We observe that the number of rings has a positive relationship with most of the rest of the variables included in the visualization. Let's explore whether these relationships are significant in determining the number of rings in an abalone and, consequently, their age.

Now that we've seen the relationship between the number of rings in an abalone and the rest of the features, let's see if we can create a model that can accurately predict the number of rings in an abalone, and consequently, its age.

First, let's use all of the features:
```{r, warning=FALSE}
multiabalone = lm(rings ~ ., data = abalone)
summary(multiabalone)
```
Almost all our predictors appear to be significant (length could be a case of multicollinearity), and we get an R-squared of ~0.54.

We then use the cross validation formula to assess the accuracy of our model:
```{r, warning=FALSE}
loocv = function(fit) {
  h = lm.influence(fit)$h
  mean((residuals(fit) / (1 - h)) ^ 2)
}
loocv(multiabalone)
```
which yields a ~4.91 MSE for the model. Not too bad! In the next insights, we will analyze the residuals and figure out if we can improve our model with different model-selection methods.

## **Classification**

###**[LDA](https://data.world/ananya-kaushik/f-17-eda-project-5/insights/5ffe1d7d-d8b7-4750-8d8e-ca272e3a0e0c)**

Here we have shellweight predicting on the count of rings bucketed into groups. The higher the count gets, the fewer data points there are, implying the old game ones are rare. Interestingly though is the decreasing weight, implying that significantly older abalone downsize.

```{r, warning = FALSE}
catabalone <- abalone
ringbuckets = cut(abalone$rings, c(1, 5, 10, 15, 30), right = FALSE, labels = c("1 - 5", "6 - 10", "11 - 15", "> 15"))

#summary(ringbuckets)

catabalone$wholeweight <- as.factor(wholeweight)
catabalone$rings <- ringbuckets
attach(catabalone)

summary(as.factor(sex))

lda.fit = lda(rings ~ shellweight, data = catabalone)
lda.pred = predict(lda.fit)
df1 = data.frame(lda.pred)

#summary(df1)
renderPlot({
  ggplot(df1) + geom_histogram(mapping = aes(x=LD1)) + facet_wrap(~ class)
})
```

With a mean of .6696, LDA is a fairly good predictor of count of abalone rings. The extreme sides are fairly inaccurate, with low and high number of rings being inaccurate. While this is not ideal, the useful part of the data for commercial purposes is focused in the middle, so this is an acceptable mistake.
[Confusion Matrix:](https://data.world/ananya-kaushik/f-17-eda-project-5/insights/3240e060-4535-4480-8bdf-f1c0f597fb32)

```{r, warning = FALSE}
table(lda.pred$class, catabalone$rings)
mean(lda.pred$class==catabalone$rings)
```
###**[QDA](https://data.world/ananya-kaushik/f-17-eda-project-5/insights/a4eeec4e-42a5-4338-96cc-f7f0d2ba539b)**

This graph is the number of rings predicted solely off of shuckedweight (the weight of the meat).

```{r, warning = FALSE}
qda.fit = qda(rings ~ shuckedweight, data = catabalone)
qda.pred = predict(qda.fit)
df2 = data.frame(qda.pred)

#summary(df2)

renderPlot({
  ggplot(data=df2, aes(catabalone$rings)) + geom_bar()
})

table(qda.pred$class, catabalone$rings)
mean(qda.pred$class==catabalone$rings)
```

With a mean of .6662, LDA and QDA perform very similarly in this case. Again, the edge cases of 1 - 5 and > 15 are fairly inaccurate, but this is inconsequential to the purpose of harvesting abalone.

## **[KNN](https://data.world/ananya-kaushik/f-17-eda-project-5/insights/bc6d8565-d224-4d64-b38e-788e4e065cdc)**

After exploring the data, we wanted to use KNN classification to predict on an abalone's number of rings. We bucketed these numbers into four categories: 0 to 5, 5 to 10, 10 to 15, and greater than 15. These buckets reflect a somewhat normal distribution, but with a slight skew to the left. The buckets are represented below:

```{r, warning = FALSE}
ringbuckets = cut(abalone$rings, c(1, 5, 10, 15, 30), right = FALSE, labels = c("1 - 5", "6 - 10", "11 - 15", "> 15"))
renderPlot({
  ggplot(abalone, aes(x=ringbuckets)) + geom_histogram(stat="count", colour="black", fill="blue", binwidth=.05)
})
```

Our dataset contains 4,177 instances, and we chose to use a little over half - 2,500 to be exact - the data as test data, while using the remainder as training data. We created a vector containing the variables 'diameter' and 'wholeweight' to predict the number of rings. We tested reducing the number of variables as well and didn't see a huge change in the accuracy of our KNN predictions. This is likely due to the fact that 'diameter' and 'wholeweight' are positively correlated with each other. Interestingly, however, adding 'sex' as a predicting variable also did not change our results significantly.

We tested the KNN function using 1, 5, 20, and 100 nearest neighbors. The following represents our confusion matrix using 1 nearest neighbor:

```{r, warning = FALSE}
vars = cbind(diameter, wholeweight)
df.subset <- vars
#df.subset

# Creating test and train subsets
test <- 1:2500
df.train <- df.subset[-test,]
df.test <- df.subset[test,]

train.def <- ringbuckets[-test]
test.def <- ringbuckets[test]


# Fitting the models
knn.1 <-  knn(df.train, df.test, train.def, k=1)
knn.5 <-  knn(df.train, df.test, train.def, k=5)
knn.20 <- knn(df.train, df.test, train.def, k=20)
knn.100 <- knn(df.train, df.test, train.def, k=100)

table(knn.1,test.def)
mean(knn.1==test.def)
```

This returned a mean of 53.84% indicating that the predictions using one neighbor are only accurate a little over half the time.

Using 5 nearest neighbors increased the mean to 58.88%, and using 20 nearest neighbors increased it to 64.2%. That's almost a 10% increase from using 1 nearest neighbor. Here is our confusion matrix for 20 nearest neighbors:

```{r}
table(knn.20,test.def)
mean(knn.20==test.def)
```

We conclude that using 100 nearest neighbors does not increase accuracy significantly, providing a mean of 65.04%, less than 1% greater than using 20 neighbors.

## **[Forward Stepwise Model Selection Using Cross Validation](https://data.world/ananya-kaushik/f-17-eda-project-5/insights/d2e026a7-a104-4592-be49-dd4e06d569c9)**

We will now use a forward stepwise method to create a model to accurate predict the number of rings in abalones. We will use cross-validation to get better results. More specifically, we will use the k-folds method, with k = 10. This is the last call for the model (k = 10).
After calculating the RMSE for every fold, we can graph it against each of the predictors used, and identify at what point we notice diminishing returns:
```{r definition}
predict.regsubsets= function(object,newdata,id,...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  mat[,names(coefi)]%*%coefi
}
```

We observe that shellweight was used at every step of the model, so we can conclude that is one of the most important variables.

```{r}
set.seed(11)
train = sample(1:nrow(abalone), 3000)
abaloneregfit = regsubsets(rings ~ ., data = abalone[train,], nvmax = 9, method = "forward")
summary(abaloneregfit)
val.errors=rep(NA,9)
x.test=model.matrix(rings~.,data=abalone[-train,])# notice the -index!
for(i in 1:9){
  coefi=coef(abaloneregfit,id=i)
  pred=x.test[,names(coefi)]%*%coefi
  val.errors[i]=mean((abalone$rings[-train]-pred)^2)
}
renderPlot({
  plot(sqrt(val.errors),ylab="Root MSE",ylim=c(2, 3),pch=19,type="b")
  points(sqrt(abaloneregfit$rss[-1]/3000),col="blue",pch=19,type="b")
  legend("topright",legend=c("Training","Validation"),col=c("blue","black"),pch=19)
})
```

We observe that shellweight was used at every step of the model, so we can conclude that is one of the most important variables.

After calculating the RMSE for every fold, we can graph it against each of the predictors used, and identify at what point we notice diminishing returns:

## **Shrinkage**
### **[Ridge Regression](https://data.world/ananya-kaushik/f-17-eda-project-5/insights/8607a4d4-6072-4971-9405-ae3fbbaf0315)**
Ridge regression minimizes the residual sum of squares by penalizing the sum of squares of the coefficients. As lambda (the tuning parameter) gets higher, the the sum of the squares of the coefficients shrinks to 0. When lambda is 0, the coefficients are unregularized. We can see this in the following graph for 10 coefficients of the abalone data.

```{r, warning = FALSE}
x=model.matrix(rings~.-1,data=abalone) #construct a matrix of the x'es
y=abalone$rings

fit.ridge=glmnet(x,y,alpha=0)
renderPlot({
  plot(fit.ridge,xvar="lambda",label=TRUE)
})
```

Now we apply cross validation on it (the function's default is 10-fold cross validation) that plots the mean squares error against log(lambda).

```{r, warning = FALSE}
cv.ridge=cv.glmnet(x,y,alpha=0)
renderPlot({
  plot(cv.ridge)
})
```

Here, the first vertical line represents the minimum, and the second is at 1 SE (standard error) away from the minimum. We see that at the high MSE the coefficients are restricted to small values. We also see that at the beginning of the curve (where the MSE is small), the graph is not very flat. This indicates that we should find a better model for this data.


###**[Lasso](https://data.world/ananya-kaushik/f-17-eda-project-5/insights/5791694b-99fa-4c48-a976-6f9b06f56b5d)**
While ridge regression uses all the predictors for the model, the Lasso uses only a subset of the predictors. Lasso minimizes the residual sum of squares by penalizing the the absolute of the coefficients. As lambda (the tuning parameter) gets higher, some of the coefficients are restricted to exactly 0. The top of the graph shows the number of non-zero coefficients as lambda increases.

```{r, warning = FALSE}
fit.lasso=glmnet(x,y)
renderPlot({
  plot(fit.lasso,xvar="lambda",label=TRUE)
})
```

We can plot the cross validation for this as well:

```{r, warning = FALSE}
cv.lasso=cv.glmnet(x,y)
renderPlot({
  plot(cv.lasso)
})
```

We can see that the minimum error is at the model of size 7, while the error 1 SE away is at the model of size 6.

We can also see the suggested model from the coeff function is the model with 6 coefficients:
```{r, warning = FALSE}
coef(cv.lasso)
```

## **Support Vector Machines**

###**[Linear SVM](https://data.world/ananya-kaushik/f-17-eda-project-5/insights/39509c58-a88e-4c90-9ee6-da20fa891cbc)**

We use the two variables shellweight and length to see if we can predict the number of rings (and in turn, the age group) of the abalone. We bucketed the rings into to groups: "<=10" and ">10". Then we fit a linear SVM and plot the fit.

```{r, warning=FALSE}
ringbuckets2 = cut(abalone$rings, c(1, 10, 30), right = FALSE, labels = c("< 10", "> 10"))
#attach(abalone)
svmdf = data.frame(output = as.factor(ringbuckets2), shellweight = abalone$shellweight, length = abalone$length)
#plot_ly(data = svmdf, x=~length, y=~shellweight)
svmfit=svm(output~.,svmdf,kernel="linear",cost=1000,scale=FALSE)
print(svmfit)

renderPlot({
  plot(svmfit, svmdf, shellweight ~ length)
})

```

We can see a clear decision boundary based on the separating hyperplane that the svm function choses with the biggest distance between the classes. However, we see that there are 2238 support vectors, or points that are on the wrong side of the boundary or too close to the boundary. So we will see if we can find a better predictor of the number of rings.

###**[Non-linear SVM](https://data.world/ananya-kaushik/f-17-eda-project-5/insights/6af03fa6-7c2b-43d4-8693-5d6e805506bf)**

We use the same two variables used in the previous SVM model to see if we can predict the number of rings (and in turn, the age group) of the abalone. We have the rings bucketed into groups: "<=10" and ">10". Now, we fit a non-linear SVM and plot the fit to see if it performs better than the linear SVM.

```{r, warning=FALSE}
svmfit3=svm(output~.,svmdf,kernel="radial",cost=1000,scale=FALSE)
print(svmfit3)
renderPlot({
  plot(svmfit3, svmdf, shellweight ~ length)
})

```

Since the kernel was set to "radial", the svm function fits a non-linear decision boundary, that we can clearly see in the plot above. We do, however, see that there are 2186 support vectors, which is marginally better than the linear SVM's 2238. So there are still a lot of points that are on the wrong side of the boundary or too close to the boundary.


## **[Bagging, Random Forest, and Boosting](https://data.world/ananya-kaushik/f-17-eda-project-5/insights/924247fb-8d82-46d4-986b-768bba723196)**

We previously used a simple multi predictor regression model and subset selection to predict the number of rings in an abalone. Now, we use more sophisticated tree-based methods to see if we can get improvements on our predictions.

### **Random Forest and Bagging**

We start by fitting a simple random forest model:

```{r, warning=FALSE}
set.seed(1011)
train = sample(1:nrow(abalone), 2500)
rf.abalone = randomForest(rings ~ ., data = abalone, subset = train)
rf.abalone
```

Random Forest uses a random number of variables when creating the trees, and is able to figure out the MSE by using out-of-bag estimates. Here we see that the MSE already as good as the subset selection model. Let's modify the number of random variables for random forest and see if we can improve it.

```{r, warning=FALSE}
oob.err = double(8)
test.err = double(8)
for (mtry in 1:8) {
  fit = randomForest(rings ~ ., data = abalone, subset = train, mtry = mtry, ntree = 500)
  oob.err[mtry] = fit$mse[500]
  pred = predict(fit, abalone[-train,])
  test.err[mtry] = with(abalone[-train,], mean((rings - pred) ^ 2))
  #cat(mtry, " ")
}

renderPlot({
  matplot(1:mtry, cbind(test.err, oob.err), pch = 19, col = c("red", "blue"), type = "b", ylab = "Mean Squared Error")
  legend("right", legend = c("OOB", "Test"), pch = 19, col = c("red", "blue"))
})
```

The out-of-bag MSE here outperforms the subset selection model, while the test MSE remains the same. We even see the MSE increase after 2 random variables.

### **Boosting**

We now use boosting to identify what predictors are most significant and see we can get any improvement on our predictions.

```{r, warning=FALSE, fig.show='hide'}
boost.abalone = gbm(rings ~ ., data = abalone[train,], distribution = "gaussian", n.trees = 10000, shrinkage = 0.01, interaction.depth = 4)
summary(boost.abalone)

```

```{r, warning=FALSE}
renderPlot({
  summary(boost.abalone)
})
```

Boosting allows us to see what variables have higher influence in the model. Here we see that shellweight and shuckedweight have the most influence. Let's see how they influence the response variable:

```{r, warning=FALSE}
renderPlot({
  plot(boost.abalone, i = "shellweight")
})

renderPlot({
  plot(boost.abalone, i = "shuckedweight")
})
#plot(boost.abalone, i = "shuckedweight")

```

As expected, we see a positive and negative relationship for shellweight and shuckedweight, respectively. Boosting builds lots of smaller trees. Unlike random forests, each new tree in boosting tries to patch up the deficiencies of the current ensemble. Let's find out how many trees are necessary to optimize our model.
```{r, warning=FALSE}
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.abalone, newdata = abalone[-train,], n.trees = n.trees)
#dim(predmat)
berr = with(abalone[-train,], apply((predmat - rings) ^ 2, 2, mean))
renderPlot({
  plot(n.trees, berr, pch = 19, ylab = "Mean Squared Error", xlab = "# Trees", main = "Boosting Test Error")
  points(x = 2000, y = min(berr), pch = 19, col = "red")
})
```

# **Statistical Analysis - Unsupervised**
## **Clustering**

### **[K-Means Clustering](https://data.world/ananya-kaushik/f-17-eda-project-5/insights/6121e5be-3b37-4246-aacb-9b9750658f0a)**

We wanted to test out some unsupervised learning techniques to look for patterns in the data, so we decided to use 'diameter' and 'shellweight' in the k-means clustering algorithm, with k=4. The algorithm returned 4 clusters of sizes 1538, 1253, 793, 593, as well as the following cluster means and the sum of squares grouped by cluster:

```{r, warning = FALSE}
ringbuckets = cut(abalone$rings, c(1, 5, 10, 15, 30), right = FALSE, labels = c("1 - 5", "6 - 10", "11 - 15", "> 15"))
ab.km = cbind(diameter, shellweight)
#ab.km

set.seed(101)
abaloneCluster <- kmeans(ab.km, 4, nstart=20)

abaloneCluster
```

We also created a visual representation of the clustering:

```{r, warning = FALSE}
abaloneCluster$cluster <- as.factor(abaloneCluster$cluster)
renderPlot({
  ggplot(abalone, aes(x=diameter, y=shellweight, color = abaloneCluster$cluster)) + geom_point()
})
```

From this we observe that each of the clusters has a mean with increasing diameter and shellweight. We then wanted to compare this to our graph of diameter and shellweight, grouped by ringbuckets, which is shown here:

```{r, warning = FALSE}
#abaloneCluster$cluster <- as.factor(abaloneCluster$cluster)
renderPlot({
  ggplot(abalone, aes(x=diameter, y=shellweight, color= ringbuckets)) + geom_point()
})
```

The results were as follows:

```{r, warning = FALSE}
table(abaloneCluster$cluster, ringbuckets)
```

We see that, while about half the data falls into the 6-10 ringbuckets category (2022 instances), only 1253 points fall into cluster two. Meanwhile, there are 1538 points in cluster 1, but only 74 instances of the data contain between 1 and 5 rings. These kinds of results come with the realm of unsupervised learning - while we may have found clusters in the data, they won't always mean something important.


### **[Heirarchical Clustering](https://data.world/ananya-kaushik/f-17-eda-project-5/insights/9d3e8131-06b0-4e4f-9552-48dcedcbe9bc)**

Now let's try another form of unsupervised learning - hierarchical clusters. We want to see how these results differ from the kmeans clusters. Our hclust call using the mean linkage method returned the following cluster dendogram (only a subset of the data was used to generate this dendogram, as it was illegible otherwise, but the remainder of the comparison uses the entire dataset):

```{r, warning = FALSE}
set.seed(101)
ab.km.subset <- ab.km[1:400]

clusters <- hclust(dist(ab.km), method='average')
renderPlot({
  plot(clusters)
})
```

From this we can see that the best choices for number of clusters are either 4 or 5. We chose to cut the tree of at five clusters. Plotting this returned the following:

```{r, warning = FALSE}
clusterCut <- cutree(clusters, 5)
#clusterCut
renderPlot({
  plot(clusterCut)
})
```

We also wanted to compare these clusters to the ringbuckets variable to see how well the data was clustered, so we created a confusion matrix comparing the clusters to the number classified in each category:

```{r, warning = FALSE}
table(abaloneCluster$cluster, ringbuckets)
```

We observed that most of the data was classified into cluster 2, with a little less than one tenth of the data classified in cluster 1, and the remainder sprinkled among clusters 3, 4, and 5.

Finally, we wanted to create a visual representation of the clustering returned by the hierarchical clustering algorithm. We created a geom_point plot:

```{r, warning=FALSE}

renderPlot({
  
ggplot(abalone, aes(diameter, shellweight, color = ringbuckets)) + 
  geom_point(alpha = 0.4, size = 3.5) + geom_point(col = clusterCut) + 
  scale_color_manual(values = c('black', 'red', 'green', 'blue'))
})

```

The points where the inner color does not match the outer color are the ones that were classified incorrectly. We observe that there was a good distribution of clusters that were classified correctly - particularly in cluster. This may be due to the fact that the number of rings is increasing as diameter and shellweight increase.

## **[Principle Component Variables](https://data.world/ananya-kaushik/f-17-eda-project-5/insights/a9277472-afad-4e9c-a5ea-eff368090af1) **

We wanted to find the vectors that explain the most variability in the dataset. Through this unsupervised learning technique, we may be able to minimize loss of information. We applied PCA to the 7 continuous variables. Our call returned the following standard deviations and principal components:
  
```{r, warning = FALSE}
ab.continuous = cbind(length, diameter, height, wholeweight, shuckedweight, visceraweight, shellweight)
#ab.continuous = cbind(as.numeric(as.factor(length)), as.numeric(as.factor(diameter)), as.numeric(as.factor(height)), as.numeric(as.factor(wholeweight)), as.numeric(as.factor(shuckedweight)), as.numeric(as.factor(visceraweight)), as.numeric(as.factor(shellweight)))
#dfab <- data.frame(list(length, diameter, height, wholeweight, shuckedweight, visceraweight, shellweight))
dfab <- data.frame(length = abalone$length, diameter = abalone$diameter, height = abalone$height, wholeweight = abalone$wholeweight, shuckedweight = abalone$shuckedweight, visceraweight = abalone$visceraweight, shellweight = abalone$shellweight)
ab.pca <- prcomp(dfab, center = TRUE, scale. = TRUE)
print(ab.pca)
```

We observe that the first principal component explains a majority of the variability in the data for most of these variables. We also wanted to visualize and summarize the results of our analysis.

```{r, warning=FALSE}
renderPlot({
  plot(ab.pca, type="l")
})
summary(ab.pca)
```

The results of the plot further support our conclusion that a large chunk of the variability in the data is explained by the first principal component. The summary method describes the importance of each of the principal components in terms of their standard deviations and the proportion of variance explained. We observe that almost 91% of the variance is explained by PC1.

Finally, we wanted to create a biplot to visually represent our findings

```{r, warning=FALSE}
renderPlot({
  biplot(ab.pca)
})
```

This biplot shows that most of these variables are clustered in the bottom left, and are very similar in terms of their variability.

