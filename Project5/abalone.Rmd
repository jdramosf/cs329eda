---
title: "Data Analytics Final Project: Abalone"
author: "Ananya Kaushik, Natasha Pirani, Nolan Bentley, and Juan Ramos Fuentes"
resource_files:
- .Renviron
output:
  html_notebook:
    code_folding: hide
    toc: yes
    toc_depth: 4
    toc_float: yes
runtime: shiny
---

```{r setup, include=FALSE}
dwapi::configure(auth_token = Sys.getenv("DW_API"))
library(tidyverse)
library(class)
require(data.world)
require(ggplot2)
require(shiny)
require(dplyr)
require(ISLR)
require(vcd)
require(plotly)
require(boot)
require(leaps)
require(tree)
require(randomForest)
require(gbm)
packageVersion('plotly')
knitr::opts_chunk$set(echo = TRUE)
options(shiny.sanitize.errors = FALSE)
```

## **Setting Up the Environment**  
### **R Session Info**
```{r, echo=FALSE, message=FALSE, warning=FALSE}
sessionInfo()
```

### **data.world Project Link**
https://data.world/ananya-kaushik/f-17-eda-project-5

### **Connecting to data.world**
```{r, echo=TRUE, message=FALSE, warning=FALSE}
project = "https://data.world/ananya-kaushik/f-17-eda-project-5"
abalone <- data.world::query(
  data.world::qry_sql("select * from abalone_data"),
  dataset = project
)

colnames(abalone) <- c("sex", "length", "diameter", "height", "wholeweight", "shuckedweight", "visceraweight", "shellweight", "rings")
abalone$sex = as.factor(abalone$sex)
attach(abalone)
```

# **Introduction**
This document contains an analysis on abalones. We used different statistical analysis methods to predict the number of rings in an abalone using different physical measures. The number of rings plus 1.5 gives the age of the abalone in years. The age of abalone is usually determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a boring and time-consuming task. We hope that, through our analysis, we can simplify this process.

## **Dataset -  Abalone**
```{r, warning=FALSE}
renderDataTable( options = list(pageLength = 10), {
  abalone
})
```


# **Statistical Analysis - Supervised**
## **Multi-predictor Linear Regression Using Cross Validation**

Let's see if we can identify strong relationships between the number of rings found in an abalone and the rest of features collected on this dataset

```{r, warning=FALSE}
renderPlot({
  pairs(abalone[2:9])
})
```

We exclude sex because is a qualitative variable.
We observe that the number of rings has a positive relationship with most of the rest of the variables included in the visualization. Let's explore whether these relationships are significant in determining the number of rings in an abalone and, consequently, their age.

Now that we've seen the relationship between the number of rings in an abalone and the rest of the features, let's see if we can create a model that can accurately predict the number of rings in an abalone, and consequently, its age.

First, let's use all of the features:
```{r, warning=FALSE}
multiabalone = lm(rings ~ ., data = abalone)
summary(multiabalone)
```
Almost all our predictors appear to be significant (length could be a case of multicollinearity), and we get an R-squared of ~0.54.

We then use the cross validation formula to assess the accuracy of our model:
```{r, warning=FALSE}
loocv = function(fit) {
  h = lm.influence(fit)$h
  mean((residuals(fit) / (1 - h)) ^ 2)
}
loocv(multiabalone)
```
which yields a ~4.91 MSE for the model. Not too bad! In the next insights, we will analyze the residuals and figure out if we can improve our model with different model-selection methods.

## **KNN**

After exploring the data, we wanted to use KNN classification to predict on an abalone's number of rings. We bucketed these numbers into four categories: 0 to 5, 5 to 10, 10 to 15, and greater than 15. These buckets reflect a somewhat normal distribution, but with a slight skew to the left. The buckets are represented below:

```{r, warning = FALSE}
ringbuckets = cut(abalone$rings, c(1, 5, 10, 15, 30), right = FALSE, labels = c("1 - 5", "6 - 10", "11 - 15", "> 15"))
renderPlot({
  ggplot(abalone, aes(x=ringbuckets)) + geom_histogram(stat="count", colour="black", fill="blue", binwidth=.05)
})
```

Our dataset contains 4,177 instances, and we chose to use a little over half - 2,500 to be exact - the data as test data, while using the remainder as training data. We created a vector containing the variables 'diameter' and 'wholeweight' to predict the number of rings. We tested reducing the number of variables as well and didn't see a huge change in the accuracy of our KNN predictions. This is likely due to the fact that 'diameter' and 'wholeweight' are positively correlated with each other. Interestingly, however, adding 'sex' as a predicting variable also did not change our results significantly.

We tested the KNN function using 1, 5, 20, and 100 nearest neighbors. The following represents our confusion matrix using 1 nearest neighbor:

```{r, warning = FALSE}
vars = cbind(diameter, wholeweight)
df.subset <- vars
#df.subset

# Creating test and train subsets
test <- 1:2500
df.train <- df.subset[-test,]
df.test <- df.subset[test,]

train.def <- ringbuckets[-test]
test.def <- ringbuckets[test]


# Fitting the models
knn.1 <-  knn(df.train, df.test, train.def, k=1)
knn.5 <-  knn(df.train, df.test, train.def, k=5)
knn.20 <- knn(df.train, df.test, train.def, k=20)
knn.100 <- knn(df.train, df.test, train.def, k=100)

table(knn.1,test.def)
mean(knn.1==test.def)
```

## **Forward Stepwise Model Selection Using Cross Validation**

We will now use a forward stepwise method to create a model to accurate predict the number of rings in abalones. We will use cross-validation to get better results. More specifically, we will use the k-folds method, with k = 10. This is the last call for the model (k = 10):
```{r definition}
predict.regsubsets= function(object,newdata,id,...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  mat[,names(coefi)]%*%coefi
}
```

```{r}
set.seed(11)
train = sample(1:nrow(abalone), 3000)
abaloneregfit = regsubsets(rings ~ ., data = abalone[train,], nvmax = 9, method = "forward")
val.errors=rep(NA,9)
x.test=model.matrix(rings~.,data=abalone[-train,])# notice the -index!
for(i in 1:9){
  coefi=coef(abaloneregfit,id=i)
  pred=x.test[,names(coefi)]%*%coefi
  val.errors[i]=mean((abalone$rings[-train]-pred)^2)
}
renderPlot({
  plot(sqrt(val.errors),ylab="Root MSE",ylim=c(2, 3),pch=19,type="b")
  points(sqrt(abaloneregfit$rss[-1]/3000),col="blue",pch=19,type="b")
  legend("topright",legend=c("Training","Validation"),col=c("blue","black"),pch=19)
})
```

We observe that shellweight was used at every step of the model, so we can conclude that is one of the most important variables.

After calculating the RMSE for every fold, we can graph it against each of the predictors used, and identify at what point we notice diminishing returns:


## **Bagging, Random Forest, and Boosting**

We previously used a simple multi predictor regression model and subset selection to predict the number of rings in an abalone. Now, we use more sophisticated tree-based methods to see if we can get improvements on our predictions.

### **Random Forest and Bagging**

We start by fitting a simple random forest model:

```{r, warning=FALSE}
set.seed(1011)
train = sample(1:nrow(abalone), 2500)
rf.abalone = randomForest(rings ~ ., data = abalone, subset = train)
rf.abalone
```

Random Forest uses a random number of variables when creating the trees, and is able to figure out the MSE by using out-of-bag estimates. Here we see that the MSE already as good as the subset selection model. Let's modify the number of random variables for random forest and see if we can improve it.

```{r, warning=FALSE}
oob.err = double(8)
test.err = double(8)
for (mtry in 1:8) {
  fit = randomForest(rings ~ ., data = abalone, subset = train, mtry = mtry, ntree = 500)
  oob.err[mtry] = fit$mse[500]
  pred = predict(fit, abalone[-train,])
  test.err[mtry] = with(abalone[-train,], mean((rings - pred) ^ 2))
  #cat(mtry, " ")
}

renderPlot({
  matplot(1:mtry, cbind(test.err, oob.err), pch = 19, col = c("red", "blue"), type = "b", ylab = "Mean Squared Error")
  legend("right", legend = c("OOB", "Test"), pch = 19, col = c("red", "blue"))
})
```

The out-of-bag MSE here outperforms the subset selection model, while the test MSE remains the same. We even see the MSE increase after 2 random variables.

### **Boosting**

We now use boosting to identify what predictors are most significant and see we can get any improvement on our predictions.

```{r, warning=FALSE, fig.show='hide'}
boost.abalone = gbm(rings ~ ., data = abalone[train,], distribution = "gaussian", n.trees = 10000, shrinkage = 0.01, interaction.depth = 4)
summary(boost.abalone)

```

```{r, warning=FALSE}
renderPlot({
  summary(boost.abalone)
})
```

Boosting allows us to see what variables have higher influence in the model. Here we see that shellweight and shuckedweight have the most influence. Let's see how they influence the response variable:

```{r, warning=FALSE}
renderPlot({
  plot(boost.abalone, i = "shellweight")
})

renderPlot({
  plot(boost.abalone, i = "shuckedweight")
})
#plot(boost.abalone, i = "shuckedweight")

```

As expected, we see a positive and negative relationship for shellweight and shuckedweight, respectively. Boosting builds lots of smaller trees. Unlike random forests, each new tree in boosting tries to patch up the deficiencies of the current ensemble. Let's find out how many trees are necessary to optimize our model.
```{r, warning=FALSE}
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.abalone, newdata = abalone[-train,], n.trees = n.trees)
dim(predmat)
berr = with(abalone[-train,], apply((predmat - rings) ^ 2, 2, mean))
renderPlot({
  plot(n.trees, berr, pch = 19, ylab = "Mean Squared Error", xlab = "# Trees", main = "Boosting Test Error")
  points(x = 2000, y = min(berr), pch = 19, col = "red")
})
```

```{r, warning=FALSE}
renderPlot({
  summary(boost.abalone)
})
```