---
title: 'Data Analytics Project 4: Bank Marketing'
author: "Ananya Kaushik, Natasha Pirani, Nolan Bentley, and Juan Ramos Fuentes"
resource_files:
- .Renviron
output:
  html_notebook:
    code_folding: hide
    toc: yes
    toc_depth: 4
    toc_float: yes
runtime: shiny
---

```{r setup, include=FALSE}
require(MASS)
require(ISLR)
require(ggplot2)
require(data.world)
require(plotly)
require(dplyr)
require(leaps)
require(tidyverse)
require(shiny)
require(class)
require(e1071)
require(tree)
require(randomForest)
knitr::opts_chunk$set(echo = TRUE)
options(shiny.sanitize.errors = FALSE)
```

# **Setting Up the Environment**  
## **R Session Info**
```{r, echo=FALSE, message=FALSE, warning=FALSE}
sessionInfo()
```


## **Connecting to data.world**
https://data.world/natashapirani96/f-17-eda-project-4
```{r, echo=FALSE, message=FALSE, warning=FALSE}
source("data.R", echo = FALSE)
```

# **Introduction** 
This document contains an analysis on the marketing campaigns of a Portuguese banking institution that used phone calls.

## **Dataset -  Bank Marketing**
```{r, warning=FALSE}
#source("analysis.R", echo = FALSE)
renderDataTable( options = list(pageLength = 10), {
  bank
})
```

# **Statistical Analysis**
## **Decision Trees**

We want to identify the variables that accurately predict the output variable: did the client subscribe to a term deposit during the direct marketing campaign? We use a tree-based approach: decision trees.
```{r, warning = FALSE, message = FALSE}
bank$output = as.factor(bank$y)
na.omit(bank)
bank = bank %>% mutate_if(is.character, as.factor)
attach(bank)
tree.bank = tree(output ~ . - y, data = bank)
summary(tree.bank)
renderPlot({
  plot(tree.bank)
  text(tree.bank, pretty = 0)
})
tree.bank
```
We first use the entire dataset to fit the tree model. The variables actually used in tree construction were: nr_employed, duration, pdays, cons_conf_idx, and euribor3m.

We then used a random sample from the original dataset and we train the model on that training sample
```{r, message=FALSE, warning=FALSE}
set.seed(1011)
train = sample(1:nrow(bank), 2000)
tree.bank = tree(output ~ . - y, bank, subset = train)
renderPlot({
  plot(tree.bank);text(tree.bank, pretty = 0)
})
```
When we look at the confusion matrix, we get a ~90% accuracy rate
```{r, message=FALSE, warning=FALSE}
tree.pred = predict(tree.bank, bank[-train,], type="class")
with(bank[-train,], table(tree.pred, output))
(1855+72)/2119
```

##**Support Vector Machines**

We bucketed median income into "<= 50,000" and "> 50,000" Then we used pctbachdeg25_over to predict median income.
```{r, message=FALSE, warning=FALSE}

```

Although the percentage of correct predictions is acceptable (around 76.5%), this does not mean pctbachdeg25_over is a good predictor. The variable predicts the income bucket of "<= 50,000" well, but it does not predict "> 50,000" well. The high mean comes from the large number of instances of median income <= 50,000.

##**Unsupervised Learning**
###**Principal Components Analysis**

To explore better predictors, we used pcths25_over with pctbachdeg25_over to predict median income. "pcths25_over" deals with the percent of county residents ages 25 and over whose highest level of education is high school.
```{r}

```
The variable similarly produces a percentage of current predictions of around 77% but does not predict the "> 50,000" bucket well.
```{r, message=FALSE}

```

###**K Means Clustering**

Since only considering the education did not seem to produce good predictions, we decided to include percentage of married people because it seemed to be somewhat correlated to median income. So we predicted using the variables pctmarriedhouseholds, pctbachdeg25_over, and pcths25_over.
```{r}


```

This produces a more accurate prediction for both buckets, "<= 50,000" and "> 50,000". The percentage of correct predictions is around 82%.


```{r, message=FALSE, warning=FALSE}

```



