---
title: 'Data Analytics Project 4: Bank Marketing'
author: "Ananya Kaushik, Natasha Pirani, Nolan Bentley, and Juan Ramos Fuentes"
resource_files:
- .Renviron
output:
  html_notebook:
    code_folding: hide
    toc: yes
    toc_depth: 4
    toc_float: yes
runtime: shiny
---

```{r setup, include=FALSE}
require(MASS)
require(ISLR)
require(ggplot2)
require(data.world)
require(plotly)
require(dplyr)
require(leaps)
require(tidyverse)
require(shiny)
require(class)
require(e1071)
require(tree)
require(randomForest)
knitr::opts_chunk$set(echo = TRUE)
options(shiny.sanitize.errors = FALSE)
```

# **Setting Up the Environment**  
## **R Session Info**
```{r, echo=FALSE, message=FALSE, warning=FALSE}
sessionInfo()
```


## **Connecting to data.world**
https://data.world/natashapirani96/f-17-eda-project-4
```{r, echo=FALSE, message=FALSE, warning=FALSE}
source("data.R", echo = FALSE)
```

# **Introduction** 
This document contains an analysis on the marketing campaigns of a Portuguese banking institution that used phone calls.

## **Dataset -  Bank Marketing**
```{r, warning=FALSE}
#source("analysis.R", echo = FALSE)
renderDataTable( options = list(pageLength = 10), {
  bank
})
```

# **Statistical Analysis**
## **Decision Trees**

We want to identify the variables that accurately predict the output variable: did the client subscribe to a term deposit during the direct marketing campaign? We use a tree-based approach: decision trees.
```{r, warning = FALSE, message = FALSE}
bank$output = as.factor(bank$y)
na.omit(bank)
bank = bank %>% mutate_if(is.character, as.factor)
attach(bank)
tree.bank = tree(output ~ . - y, data = bank)
summary(tree.bank)
renderPlot({
  plot(tree.bank)
  text(tree.bank, pretty = 0)
})
tree.bank
```
We first use the entire dataset to fit the tree model. The variables actually used in tree construction were: nr_employed, duration, pdays, cons_conf_idx, and euribor3m.

We then used a random sample from the original dataset and we train the model on that training sample
```{r, message=FALSE, warning=FALSE}
set.seed(1011)
train = sample(1:nrow(bank), 2000)
tree.bank = tree(output ~ . - y, bank, subset = train)
renderPlot({
  plot(tree.bank);text(tree.bank, pretty = 0)
})
```
When we look at the confusion matrix, we get a ~90% accuracy rate
```{r, message=FALSE, warning=FALSE}
tree.pred = predict(tree.bank, bank[-train,], type="class")
with(bank[-train,], table(tree.pred, output))
(1855+72)/2119
```
We will explore different tree-based methods to see if we can improve our predictive model.

###**Random Forest and Bagging**

First we implemented a cross-validation method for decision trees. We see that the number of misclassifications decreases as the size of the tree increases - to a certain point
```{r, message=FALSE, warning=FALSE}
cv.bank = cv.tree(tree.bank, FUN = prune.misclass)
cv.bank
renderPlot({
  plot(cv.bank)
})
```

We then pruned the tree to that size
```{r, message=FALSE, warning=FALSE}
prune.bank = prune.misclass(tree.bank, best = 8)
renderPlot({
  plot(prune.bank);text(prune.bank, pretty = 0)
})
tree.pred = predict(prune.bank, bank[-train,], type="class")
with(bank[-train,], table(tree.pred, output))
(1850+88)/2119
```

The accuracy of our model has improved! But misclassifications of true positives remain high. Can we do better?

We then used random forest with a training subset of 2000 records

```{r, message=FALSE, warning=FALSE}
set.seed(101)
dim(bank)
train = sample(1:nrow(bank), 2000)

rf.bank = randomForest(output ~ .-y, data = bank, subset = train)
rf.bank
```
Much much better! Our predictions for true positives are much more accurate! We will explore more tree-based models in future analysis.

##**Support Vector Machines**

Here, a model was created using the output (target) variable "y", duration (of the phone call), and consumer confidence index (CCI). This variable "y" is a boolean, indicating whether or not the client has subscribed to the term deposit.

```{r, message=FALSE, warning=FALSE}
bank = na.omit(bank)
df = data.frame(output = as.factor(bank$y), CCI = bank$cons_conf_idx, duration = bank$duration)
svmfit=svm(output~.,df,kernel="linear",cost=1000,scale=FALSE)
svmfit
```

The plot of the linear support vector machine was created:

```{r, message=FALSE, warning=FALSE}
renderPlot({
  plot(svmfit, df, CCI ~ duration)
})
```

This plots duration on the X-axis and CCI on the Y-axis, separated by output "y".

##**Unsupervised Learning**
###**Principal Components Analysis**

We wanted to do a Principal Component Analysis using the numerical variables. We began by defining a subset of the data that only included numerical variables. We then used the princomp() function to obtain the standard deviations of each component. We observed 10 components:
```{r, message=FALSE, warning=FALSE}
nums <- sapply(df, is.numeric)
df1 <- df[ , nums]

#print(df1)

princomp(df1)


``` 

We then created a summary of the output of this function, through which we observed that 95.7% of variation can be attributed to components 1 and 2:

```{r, message=FALSE, warning=FALSE}
bank.PCA <- princomp(df1)
summary(bank.PCA)
```

We were also able to observe the factor loadings for each component, which allow us to determine the correlation coefficients between the factors and the variables:


```{r, message=FALSE, warning=FALSE}
bank.PCA$loadings
renderPlot({
  biplot(bank.PCA, scale=1)
})
```

Through this, we observed a strongly negative correlation between age and component 4, duration and component 1, and pdays and component 2. Our strongest positive correlation was observed between component for and nr_employed, which is the average number of employed citizens.


###**K Means Clustering - Interesting Analysis!**

We wanted to perform a K-Means Clustering analysis, so we began by doing some exploratory analysis of the data to see if we found any clusters. We created a scatterplot using age to predict consumer price index, and separated the colors based on the job of the client:

```{r, message=FALSE, warning=FALSE}

renderPlot({
  ggplot(bank, aes(x=age, y=cons_price_idx, color= job)) + geom_point()
})

```

We found an interesting distribution of jobs, but not very much clustering besides those who were retired, so we decided to test the relationship between age and consumer price index, while separating by consumer confidence index:

```{r, message=FALSE, warning=FALSE}

renderPlot({
  ggplot(bank, aes(x=age, y=cons_price_idx, color= cons_conf_idx)) + geom_point()
})

```

We found that consumer confidence index appears to increase with consumer price index, so we decided to perform k-means clustering using these variables. We set the seed to 101, and used the kmeans function, which returned the following means:

```{r, message=FALSE, warning=FALSE}
set.seed(101)
bankCluster <- kmeans(df1, 3, nstart=20)
bankCluster
```
Because this is a form of unsupervised learning, we cannot establish how conclusive these results are, but for age, consumer price index and consumer confidence index, we observe means that do not differ greatly based on the size of the cluster, as opposed to other factors such as duration, which differ greatly based on size
